{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24db587424c051ff",
   "metadata": {},
   "source": [
    "# Testing LoRa with GPT2-Medium for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651ce240c07d5fd",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:00:51.342824Z",
     "start_time": "2024-10-04T10:00:51.317785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gpt2_medium import Trainer,SimpleTrainer\n",
    "from labml import experiment\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fe43db1226960",
   "metadata": {},
   "source": [
    "## Check use of GPU, this could be performed with CPU, however it will take more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8630478fc30b2881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T20:13:31.000514Z",
     "start_time": "2024-10-03T20:13:30.446530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b4ccb0e9e4bc3b36625d614d742f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "Executing with cuda (GPU)\n"
     ]
    }
   ],
   "source": [
    "experiment.create(name=\"lora_gpt2\")\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Executing with cuda (GPU)\")\n",
    "else:\n",
    "    print(\"Not using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c4c9dd6889309",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "\n",
    "Here we are loading all the information about the model, the iterations and batch size for LoRa as well as the \"r\" used for the A and B submatrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655ab91f22992216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:00:46.701566Z",
     "start_time": "2024-10-04T10:00:46.009380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40540dee80f44b4bd264a45497bf89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"></pre>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer_no_lora=SimpleTrainer()\n",
    "\n",
    "experiment.configs(trainer)\n",
    "\n",
    "trainer_no_lora.initialize()\n",
    "trainer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392a8e1eca44b6",
   "metadata": {},
   "source": [
    "## Start the experiment\n",
    "In this section we will compare simple finetuning vs LoRa in different aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ab7f60f2b3bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82228b96b094272a2464f731acaa7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><strong><span style=\"color: #DDB62B\">       0:  </span></strong>…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\labml_nn\\lora\\gpt2.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LoRa rank: 1\n",
      "Training with LoRa rank: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simple_finetune_results = trainer.simple_finetuning()\n",
    "lora_results = trainer.lora_finetuning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368039956141ecf1",
   "metadata": {},
   "source": [
    "## Compare the results when modifying R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f74c553cf20c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot improvements with increasing R\n",
    "trainer.plot_lora_improvement(lora_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeaf73baabb616b",
   "metadata": {},
   "source": [
    "## Compare the results between LoRa and simple FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b938294601e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison between LoRa and simple fine-tuning\n",
    "trainer.plot_finetune_vs_lora(lora_results, simple_finetune_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b243465cd49cb12",
   "metadata": {},
   "source": [
    "## Compare the time needed for LoRa vs simple FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819edf7c7d9822fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time comparison between LoRa and fine-tuning\n",
    "trainer.plot_time_comparison(lora_results, simple_finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6b263cde6ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
